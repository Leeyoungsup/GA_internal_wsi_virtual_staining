{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "import itertools\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image\n",
    "import random\n",
    "import matplotlib.pyplot as plt \n",
    "topilimage =transforms.ToPILImage()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def create_dir(path):\n",
    "    import os\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model params\n",
    "params = {\n",
    "    'batch_size':4,\n",
    "    'input_size':512,\n",
    "    'resize_scale':512,\n",
    "    'crop_size':512,\n",
    "    'fliplr':False,\n",
    "    'num_epochs':100,\n",
    "    'decay_epoch':50,\n",
    "    'ngf':32,   #number of generator filters\n",
    "    'ndf':64,   #number of discriminator filters\n",
    "    'num_resnet':6, #number of resnet blocks\n",
    "    'lrG':2e-5,    #learning rate for generator\n",
    "    'lrD':2e-5,    #learning rate for discriminator\n",
    "    'beta1':0.5 ,    #beta1 for Adam optimizer\n",
    "    'beta2':0.999 ,  #beta2 for Adam optimizer\n",
    "    'lambdaA':10 ,   #lambdaA for cycle loss\n",
    "    'lambdaB':10,  #lambdaB for cycle loss\n",
    "    'img_form':'jpg'\n",
    "}\n",
    "\n",
    "data_dir = '../../data/IHC_HE_Pair_Data_GA_SS/patches/'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image step by step\n",
    "def to_np(x):\n",
    "    return x.data.cpu().numpy()\n",
    "class ImagePool():\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        if self.pool_size > 0:\n",
    "            self.num_imgs = 0\n",
    "            self.images = []\n",
    "\n",
    "    def query(self, images):\n",
    "        if self.pool_size == 0:\n",
    "            return images\n",
    "        return_images = []\n",
    "        with torch.no_grad():  # No need to track gradients\n",
    "            for image in images:\n",
    "                image = torch.unsqueeze(image, 0)\n",
    "                if self.num_imgs < self.pool_size:\n",
    "                    self.num_imgs += 1\n",
    "                    self.images.append(image)\n",
    "                    return_images.append(image)\n",
    "                else:\n",
    "                    p = random.uniform(0, 1)\n",
    "                    if p > 0.5:\n",
    "                        random_id = random.randint(0, self.pool_size - 1)\n",
    "                        tmp = self.images[random_id].clone()\n",
    "                        self.images[random_id] = image\n",
    "                        return_images.append(tmp)\n",
    "                    else:\n",
    "                        return_images.append(image)\n",
    "            return_images = torch.cat(return_images, 0)\n",
    "        return return_images.detach()  \n",
    "        \n",
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, HE_image_list,IHC_image_list):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.HE_image_list =HE_image_list\n",
    "        self.IHC_image_list =IHC_image_list\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Load Image\n",
    "        img = Image.open(self.HE_image_list[index]).convert('RGB')\n",
    "        width, height = img.size\n",
    "        random_crop_x = random.randint(0, max(0, width - params['crop_size'] - 1))\n",
    "        random_crop_y = random.randint(0, max(0, height - params['crop_size'] - 1))\n",
    "        img = img.crop((random_crop_x, random_crop_y, random_crop_x + params['crop_size'], random_crop_y + params['crop_size']))\n",
    "        img = transform(img)*2.-1\n",
    "        target = Image.open(self.IHC_image_list[index]).convert('RGB')\n",
    "        target = target.crop((random_crop_x, random_crop_y, random_crop_x + params['crop_size'], random_crop_y + params['crop_size']))\n",
    "        target = transform(target)*2.-1\n",
    "        return img, target\n",
    "    def __len__(self):\n",
    "        return len(self.HE_image_list)\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=params['input_size']),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "#Subfolders - day & night\n",
    "train_data_HE=glob(f'{data_dir}train/HER2/HE/*.{params[\"img_form\"]}')\n",
    "train_data_IHC=[f.replace('/HE/','/IHC/') for f in train_data_HE]\n",
    "test_data_HE=glob(f'{data_dir}test/HER2/HE/*.{params[\"img_form\"]}')\n",
    "test_data_IHC=[f.replace('/HE/','/IHC/') for f in test_data_HE]\n",
    "train_data= DatasetFromFolder(train_data_HE,train_data_IHC)\n",
    "test_data= DatasetFromFolder(test_data_HE,test_data_IHC)\n",
    "loader = torch.utils.data.DataLoader(dataset=train_data , batch_size=params['batch_size'], shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data , batch_size=1, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CycleGAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(features, features, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(features, features, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.Dropout(0.5)  # Dropout 추가 (드롭아웃 확률 0.5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# Generator Model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, n_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "        # 초기 컨볼루션 블록\n",
    "        model = [\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=7, padding=3),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        # 다운샘플링\n",
    "        in_features = 64\n",
    "        out_features = in_features * 2\n",
    "        for _ in range(4):  # 기존 2에서 4로 변경\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "\n",
    "        # 잔차 블록\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # 업샘플링\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(4):  # 기존 2에서 4로 변경\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "\n",
    "        # 출력 레이어\n",
    "        model += [nn.Conv2d(64, output_channels, kernel_size=7, padding=3), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Discriminator Model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, kernel_size=4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(input_channels, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(3, 3).to(device)  # 그레이스케일에서 컬러로\n",
    "F = Generator(3, 3).to(device)  # 컬러에서 그레이스케일로\n",
    "D_color = Discriminator(3).to(device)\n",
    "D_gray = Discriminator(3).to(device)\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer_G = optim.Adam(itertools.chain(G.parameters(), F.parameters()), lr=2e-5, betas=(0.5, 0.999))\n",
    "optimizer_D_color = optim.Adam(D_color.parameters(), lr=2e-5, betas=(0.5, 0.999))\n",
    "optimizer_D_gray = optim.Adam(D_gray.parameters(), lr=2e-5, betas=(0.5, 0.999))\n",
    "\n",
    "# 학습률 스케줄러 추가\n",
    "lr_scheduler_G = optim.lr_scheduler.StepLR(optimizer_G, step_size=20, gamma=0.5)\n",
    "lr_scheduler_D_color = optim.lr_scheduler.StepLR(optimizer_D_color, step_size=20, gamma=0.5)\n",
    "lr_scheduler_D_gray = optim.lr_scheduler.StepLR(optimizer_D_gray, step_size=20, gamma=0.5)\n",
    "\n",
    "# 손실 함수 설정\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(params['num_epochs']):\n",
    "    \n",
    "    total_loss_G=0\n",
    "    total_loss_D_color=0\n",
    "    total_loss_D_gray=0\n",
    "    count=0\n",
    "    with tqdm(loader, total=len(loader), desc=f\"Epoch {epoch+1}/{params['num_epochs']}\") as pbar:\n",
    "        for gray_img,color_img in pbar:\n",
    "            gray_img = gray_img.to(device)\n",
    "            color_img = color_img.to(device)\n",
    "\n",
    "            # 생성자 G와 F 업데이트\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # 아이덴티티 손실\n",
    "            loss_id_G = criterion_identity(G(color_img), color_img) * 5.0\n",
    "            loss_id_F = criterion_identity(F(gray_img), gray_img) * 5.0\n",
    "\n",
    "            # GAN 손실\n",
    "            fake_color = G(gray_img)\n",
    "            pred_fake = D_color(fake_color)\n",
    "            loss_GAN_G = criterion_GAN(pred_fake, torch.ones_like(pred_fake).to(device))\n",
    "\n",
    "            fake_gray = F(color_img)\n",
    "            pred_fake = D_gray(fake_gray)\n",
    "            loss_GAN_F = criterion_GAN(pred_fake, torch.ones_like(pred_fake).to(device))\n",
    "\n",
    "            # 순환 일관성 손실\n",
    "            recov_gray = F(fake_color)\n",
    "            loss_cycle_GF = criterion_cycle(recov_gray, gray_img) * 10.0\n",
    "\n",
    "            recov_color = G(fake_gray)\n",
    "            loss_cycle_FG = criterion_cycle(recov_color, color_img) * 10.0\n",
    "\n",
    "            # 총 생성자 손실\n",
    "            loss_G = loss_id_G + loss_id_F + loss_GAN_G + loss_GAN_F + loss_cycle_GF + loss_cycle_FG\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # 판별자 D_color 업데이트\n",
    "            optimizer_D_color.zero_grad()\n",
    "\n",
    "            pred_real = D_color(color_img)\n",
    "            loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real).to(device))\n",
    "\n",
    "            pred_fake = D_color(fake_color.detach())\n",
    "            loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake).to(device))\n",
    "\n",
    "            loss_D_color = (loss_D_real + loss_D_fake) * 0.5\n",
    "            loss_D_color.backward()\n",
    "            optimizer_D_color.step()\n",
    "\n",
    "            # 판별자 D_gray 업데이트\n",
    "            optimizer_D_gray.zero_grad()\n",
    "\n",
    "            pred_real = D_gray(gray_img)\n",
    "            loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real).to(device))\n",
    "\n",
    "            pred_fake = D_gray(fake_gray.detach())\n",
    "            loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake).to(device))\n",
    "\n",
    "            loss_D_gray = (loss_D_real + loss_D_fake) * 0.5\n",
    "            loss_D_gray.backward()\n",
    "            optimizer_D_gray.step()\n",
    "\n",
    "            total_loss_D_color+=loss_D_color.item()\n",
    "            total_loss_D_gray+=loss_D_gray.item()\n",
    "            total_loss_G+=loss_G.item()\n",
    "            count+=1\n",
    "            pbar.set_postfix({\n",
    "                'Loss G': f'{total_loss_G/count:.4f}',\n",
    "                'Loss D_color': f'{total_loss_D_color/count:.4f}',\n",
    "                'Loss D_gray': f'{total_loss_D_gray/count:.4f}'\n",
    "            })\n",
    "        with torch.no_grad():\n",
    "            random_idx = random.randint(0, len(test_data) - 1)\n",
    "            gray_img, color_img = test_data[random_idx]\n",
    "            gray_img = gray_img.unsqueeze(0).to(device)\n",
    "            color_img = color_img.unsqueeze(0).to(device)\n",
    "            fake_color = G(gray_img)\n",
    "            recov_gray = F(fake_color)\n",
    "        def denormalize(img):\n",
    "            return img * 0.5 + 0.5\n",
    "\n",
    "        gray_img_vis = denormalize(gray_img[0])\n",
    "        color_img_vis = denormalize(color_img[0])\n",
    "        fake_color_vis = denormalize(fake_color[0])\n",
    "        recov_gray_vis = denormalize(recov_gray[0])\n",
    "\n",
    "        # 이미지 리스트 생성\n",
    "        images = [gray_img_vis, fake_color_vis, recov_gray_vis, color_img_vis]\n",
    "\n",
    "        # 이미지들을 너비 방향으로 concatenate\n",
    "        concatenated = torch.cat(images, dim=2)  # dim=3은 너비 방향\n",
    "    create_dir('../../results/HE_IHC_translation/HER2')\n",
    "    create_dir('../../model/HE_IHC_translation/HER2')\n",
    "    save_image(concatenated, f'../../results/HE_IHC_translation/HER2/concatenated_epoch{epoch}.png')\n",
    "    torch.save(G.state_dict(), f'../../model/HE_IHC_translation/HER2/G_{epoch}.pth')\n",
    "    torch.save(F.state_dict(), f'../../model/HE_IHC_translation/HER2/F_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    random_idx = random.randint(0, len(test_data) - 1)\n",
    "    gray_img, color_img = test_data[random_idx]\n",
    "    gray_img = gray_img.unsqueeze(0).to(device)\n",
    "    color_img = color_img.unsqueeze(0).to(device)\n",
    "    fake_color = G(gray_img)\n",
    "    recov_gray = F(fake_color)\n",
    "def denormalize(img):\n",
    "    return img * 0.5 + 0.5\n",
    "\n",
    "gray_img_vis = denormalize(gray_img[0])\n",
    "color_img_vis = denormalize(color_img[0])\n",
    "fake_color_vis = denormalize(fake_color[0])\n",
    "recov_gray_vis = denormalize(recov_gray[0])\n",
    "\n",
    "# 이미지 리스트 생성\n",
    "images = [gray_img_vis, fake_color_vis, recov_gray_vis, color_img_vis]\n",
    "\n",
    "# 이미지들을 너비 방향으로 concatenate\n",
    "concatenated = torch.cat(images, dim=2)  # dim=3은 너비 방향\n",
    "plt.imshow(topilimage(concatenated.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "G.load_state_dict(torch.load('G.pth'))\n",
    "G.eval()\n",
    "\n",
    "# 테스트 이미지 로드\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "from PIL import Image\n",
    "image = Image.open('path_to_grayscale_image.jpg')\n",
    "image = test_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "# 컬러화된 이미지 생성\n",
    "with torch.no_grad():\n",
    "    fake_color = G(image)\n",
    "\n",
    "# 이미지 저장\n",
    "save_image(fake_color, 'colorized_image.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urban",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
