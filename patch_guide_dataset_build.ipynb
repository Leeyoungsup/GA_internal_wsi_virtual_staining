{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f727d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "import itertools\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image\n",
    "import random\n",
    "import matplotlib.pyplot as plt \n",
    "topilimage =transforms.ToPILImage()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bbbae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(features, features, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(features, features, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.Dropout(0.5)  # Dropout 추가 (드롭아웃 확률 0.5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# Generator Model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, n_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "        # 초기 컨볼루션 블록\n",
    "        model = [\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=7, padding=3),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        # 다운샘플링\n",
    "        in_features = 64\n",
    "        out_features = in_features * 2\n",
    "        for _ in range(4):  # 기존 2에서 4로 변경\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "\n",
    "        # 잔차 블록\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # 업샘플링\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(4):  # 기존 2에서 4로 변경\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "\n",
    "        # 출력 레이어\n",
    "        model += [nn.Conv2d(64, output_channels, kernel_size=7, padding=3), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Discriminator Model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, kernel_size=4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(input_channels, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "    \n",
    "# Initialize and load model\n",
    "F = Generator(3, 3).to(device)\n",
    "model_path = '../../model/HE_IHC_translation/internal_ss/PD-L1/F_99.pth'\n",
    "F.load_state_dict(torch.load(model_path, map_location=device))\n",
    "F.eval()\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=512),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "print(f\"✓ Loaded model: {model_path}\")\n",
    "print(f\"✓ Generator F (IHC → HE) ready for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a4378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdl1_guide_src_list=glob('../../data/IHC_HE_Pair_Data_GA_SS/patches/pdl1_mpp1/*.png')\n",
    "pdl1_guide_path='../../data/IHC_HE_Pair_Data_GA_SS/patches/pdl1_guide/'\n",
    "he_guide_path='../../data/IHC_HE_Pair_Data_GA_SS/patches/he_guide/'\n",
    "\n",
    "create_dir(pdl1_guide_path)\n",
    "create_dir(he_guide_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83502590",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(pdl1_guide_src_list))):\n",
    "\n",
    "    filename=os.path.basename(pdl1_guide_src_list[i])\n",
    "    pdl1_img=Image.open(pdl1_guide_src_list[i]).convert('RGB')\n",
    "    pdl1_tensor=transform(pdl1_img).unsqueeze(0).to(device)*2.-1.\n",
    "    with torch.no_grad():\n",
    "        he_tensor=F(pdl1_tensor)\n",
    "    he_img=he_tensor.squeeze(0).cpu()*0.5+0.5\n",
    "    pdl1_img=pdl1_tensor.squeeze(0).cpu()*0.5+0.5\n",
    "    he_np_img = np.array(he_img.permute(1, 2, 0)*255).astype(np.uint8)\n",
    "    he_with_rect = he_np_img.copy()  # 이미지 복사\n",
    "    cv2.rectangle(he_with_rect, (128,128), (384,384), (0,255,0), 2)  \n",
    "    pdl1_np_img=np.array(pdl1_img.permute(1, 2, 0)*255).astype(np.uint8)\n",
    "    pdl1_with_rect = pdl1_np_img.copy()  # 이미지 복사\n",
    "    cv2.rectangle(pdl1_with_rect, (128,128), (384,384), (0,255,0), 2)  \n",
    "    he_with_rect_img=Image.fromarray(he_with_rect)\n",
    "    pdl1_with_rect_img=Image.fromarray(pdl1_with_rect)\n",
    "    pdl1_with_rect_img.save(os.path.join(pdl1_guide_path,filename.split('.')[0]+'_pdl1_guide.tiff'))\n",
    "    he_with_rect_img.save(os.path.join(he_guide_path,filename.split('.')[0]+'_he_guide.tiff'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8c0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename.split('.')[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urban",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
