{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792535ab",
   "metadata": {},
   "source": [
    "# CycleGAN Test: IHC to HE Virtual Staining Evaluation\n",
    "\n",
    "This notebook provides comprehensive evaluation of IHC → HE translation using trained CycleGAN model.\n",
    "\n",
    "## Evaluation Metrics (Standard for Virtual Staining)\n",
    "- **PSNR**: Peak Signal-to-Noise Ratio\n",
    "- **SSIM**: Structural Similarity Index\n",
    "- **LPIPS**: Learned Perceptual Image Patch Similarity\n",
    "- **FID**: Fréchet Inception Distance (GAN standard)\n",
    "- **Color Histogram Similarity**: Critical for staining evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419d345",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f471350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models import inception_v3\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "from scipy.stats import gaussian_kde\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb0bbb",
   "metadata": {},
   "source": [
    "## 2. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lpips scikit-image -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95cd411",
   "metadata": {},
   "source": [
    "## 3. Test Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d480359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parameters\n",
    "params = {\n",
    "    'batch_size': 1,\n",
    "    'input_size': 512,\n",
    "    'test_sample_count': 100,\n",
    "    'model_epoch': 99,\n",
    "    'img_form': 'png'\n",
    "}\n",
    "\n",
    "# Paths\n",
    "data_dir = '../../data/IHC_HE_Pair_Data_GA_SS/patches'\n",
    "model_dir = '../../model/HE_IHC_translation/internal_ss/PD-L1'\n",
    "result_dir = '../../results/HE_IHC_translation/internal_ss/PD-L1/test_results'\n",
    "\n",
    "# Create directories\n",
    "create_dir(result_dir)\n",
    "create_dir(f'{result_dir}/visualizations')\n",
    "create_dir(f'{result_dir}/generated_images')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model epoch: {params['model_epoch']}\")\n",
    "print(f\"Test samples: {params['test_sample_count']}\")\n",
    "print(f\"Image size: {params['input_size']}x{params['input_size']}\")\n",
    "print(f\"Results directory: {result_dir}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d57b16",
   "metadata": {},
   "source": [
    "## 4. Define Generator Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d80b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(features, features, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(features, features, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, n_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "        model = [\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=7, padding=3),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        # Downsampling\n",
    "        in_features = 64\n",
    "        out_features = in_features * 2\n",
    "        for _ in range(4):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(4):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "\n",
    "        # Output layer\n",
    "        model += [nn.Conv2d(64, output_channels, kernel_size=7, padding=3), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "print(\"✓ Generator architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30289a69",
   "metadata": {},
   "source": [
    "## 5. Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3743a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, HE_image_list, IHC_image_list):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.HE_image_list = HE_image_list\n",
    "        self.IHC_image_list = IHC_image_list\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.HE_image_list[index], self.IHC_image_list[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.HE_image_list)\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=params['input_size']),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load test data\n",
    "print(\"Loading test data...\")\n",
    "test_data_HE = glob(f'{data_dir}/he_mpp1/*.{params[\"img_form\"]}')\n",
    "test_max_count = min(params['test_sample_count'], len(test_data_HE))\n",
    "test_data_HE = random.sample(test_data_HE, test_max_count)\n",
    "test_data_IHC = [f.replace('/he_mpp1/', '/pdl1_mpp1/') for f in test_data_HE]\n",
    "\n",
    "print(f\"Found {len(test_data_HE)} test image pairs\")\n",
    "\n",
    "# Preload images\n",
    "test_image_HE = torch.zeros((len(test_data_HE), 3, params['input_size'], params['input_size']))\n",
    "test_image_IHC = torch.zeros((len(test_data_IHC), 3, params['input_size'], params['input_size']))\n",
    "\n",
    "for i in tqdm(range(len(test_data_HE)), desc=\"Loading test images\"):\n",
    "    img = Image.open(test_data_HE[i]).convert('RGB')\n",
    "    target = Image.open(test_data_IHC[i]).convert('RGB')\n",
    "    img = transform(img) * 2. - 1\n",
    "    target = transform(target) * 2. - 1\n",
    "    test_image_HE[i] = img\n",
    "    test_image_IHC[i] = target\n",
    "\n",
    "test_dataset = DatasetFromFolder(test_image_HE, test_image_IHC)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=params['batch_size'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"✓ Test dataset loaded: {len(test_dataset)} image pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab036431",
   "metadata": {},
   "source": [
    "## 6. Load Trained Model (Generator F: IHC → HE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3084e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and load model\n",
    "F = Generator(3, 3).to(device)\n",
    "model_path = f'{model_dir}/F_{params[\"model_epoch\"]}.pth'\n",
    "F.load_state_dict(torch.load(model_path, map_location=device))\n",
    "F.eval()\n",
    "\n",
    "print(f\"✓ Loaded model: {model_path}\")\n",
    "print(f\"✓ Generator F (IHC → HE) ready for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17e71be",
   "metadata": {},
   "source": [
    "## 7. Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0977e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lpips\n",
    "\n",
    "# Initialize LPIPS\n",
    "lpips_model = lpips.LPIPS(net='alex').to(device)\n",
    "\n",
    "def denormalize(img):\n",
    "    \"\"\"Denormalize from [-1, 1] to [0, 1]\"\"\"\n",
    "    return img * 0.5 + 0.5\n",
    "\n",
    "def calculate_psnr(img1, img2):\n",
    "    img1_np = img1.cpu().numpy().transpose(1, 2, 0)\n",
    "    img2_np = img2.cpu().numpy().transpose(1, 2, 0)\n",
    "    return psnr(img1_np, img2_np, data_range=1.0)\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    img1_np = img1.cpu().numpy().transpose(1, 2, 0)\n",
    "    img2_np = img2.cpu().numpy().transpose(1, 2, 0)\n",
    "    return ssim(img1_np, img2_np, multichannel=True, data_range=1.0, channel_axis=2)\n",
    "\n",
    "def calculate_lpips(img1, img2):\n",
    "    with torch.no_grad():\n",
    "        lpips_value = lpips_model(img1.unsqueeze(0), img2.unsqueeze(0))\n",
    "    return lpips_value.item()\n",
    "\n",
    "def calculate_mae(img1, img2):\n",
    "    return torch.mean(torch.abs(img1 - img2)).item()\n",
    "\n",
    "def calculate_histogram_similarity(img1, img2, bins=256):\n",
    "    \"\"\"Calculate histogram correlation for RGB channels\"\"\"\n",
    "    correlations = []\n",
    "    for channel in range(3):\n",
    "        hist1, _ = np.histogram(img1[channel].flatten(), bins=bins, range=(0, 1))\n",
    "        hist2, _ = np.histogram(img2[channel].flatten(), bins=bins, range=(0, 1))\n",
    "        hist1 = hist1 / hist1.sum()\n",
    "        hist2 = hist2 / hist2.sum()\n",
    "        correlation = np.corrcoef(hist1, hist2)[0, 1]\n",
    "        correlations.append(correlation)\n",
    "    return np.mean(correlations)\n",
    "\n",
    "print(\"✓ Evaluation metrics initialized\")\n",
    "print(\"  - PSNR, SSIM, LPIPS, MAE\")\n",
    "print(\"  - Color Histogram Similarity\")\n",
    "print(\"  - FID (will be calculated separately)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25b430",
   "metadata": {},
   "source": [
    "## 8. Run Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f04e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for metrics\n",
    "results = {\n",
    "    'psnr': [],\n",
    "    'ssim': [],\n",
    "    'lpips': [],\n",
    "    'mae': [],\n",
    "    'hist_sim': []\n",
    "}\n",
    "\n",
    "# Storage for ALL generated images (needed for best/worst analysis)\n",
    "generated_images = []\n",
    "real_he_images = []\n",
    "input_ihc_images = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING EVALUATION ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (real_he, real_ihc) in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n",
    "        real_he = real_he.to(device)\n",
    "        real_ihc = real_ihc.to(device)\n",
    "        \n",
    "        # Generate fake HE from IHC\n",
    "        fake_he = F(real_ihc)\n",
    "        \n",
    "        # Denormalize\n",
    "        real_he_denorm = denormalize(real_he[0])\n",
    "        fake_he_denorm = denormalize(fake_he[0])\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results['psnr'].append(calculate_psnr(fake_he_denorm, real_he_denorm))\n",
    "        results['ssim'].append(calculate_ssim(fake_he_denorm, real_he_denorm))\n",
    "        results['lpips'].append(calculate_lpips(fake_he[0], real_he[0]))\n",
    "        results['mae'].append(calculate_mae(fake_he_denorm, real_he_denorm))\n",
    "        results['hist_sim'].append(calculate_histogram_similarity(fake_he_denorm.cpu(), real_he_denorm.cpu()))\n",
    "        \n",
    "        # Store all images\n",
    "        generated_images.append(fake_he_denorm.cpu())\n",
    "        real_he_images.append(real_he_denorm.cpu())\n",
    "        input_ihc_images.append(denormalize(real_ihc[0]).cpu())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Evaluation completed!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a529a3f",
   "metadata": {},
   "source": [
    "## 9. Calculate FID Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6629a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import sqrtm\n",
    "\n",
    "# Load Inception V3\n",
    "print(\"Loading Inception V3 model for FID calculation...\")\n",
    "inception_model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "inception_model.fc = nn.Identity()\n",
    "inception_model.eval()\n",
    "\n",
    "def get_inception_features(images):\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for img in tqdm(images, desc=\"Extracting features\"):\n",
    "            # Resize to 299x299 for Inception\n",
    "            img_resized = torch.nn.functional.interpolate(\n",
    "                img.unsqueeze(0), size=(299, 299), mode='bilinear', align_corners=False\n",
    "            )\n",
    "            # Normalize to [-1, 1]\n",
    "            img_normalized = img_resized * 2 - 1\n",
    "            feat = inception_model(img_normalized.to(device))\n",
    "            features.append(feat.cpu().numpy())\n",
    "    return np.concatenate(features, axis=0)\n",
    "\n",
    "def calculate_fid(real_features, fake_features):\n",
    "    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu2, sigma2 = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)\n",
    "    \n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid\n",
    "\n",
    "# Extract features\n",
    "print(\"Extracting features from real HE images...\")\n",
    "real_features = get_inception_features(real_he_images)\n",
    "\n",
    "print(\"Extracting features from generated HE images...\")\n",
    "fake_features = get_inception_features(generated_images)\n",
    "\n",
    "# Calculate FID\n",
    "fid_score = calculate_fid(real_features, fake_features)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"FID Score: {fid_score:.2f}\")\n",
    "print(\"=\"*60)\n",
    "print(\"Interpretation:\")\n",
    "print(\"  - FID < 50: Excellent\")\n",
    "print(\"  - FID 50-100: Good\")\n",
    "print(\"  - FID 100-200: Moderate\")\n",
    "print(\"  - FID > 200: Poor\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5162b62d",
   "metadata": {},
   "source": [
    "## 10. Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d421f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "statistics = {}\n",
    "for metric_name, values in results.items():\n",
    "    statistics[metric_name] = {\n",
    "        'mean': np.mean(values),\n",
    "        'std': np.std(values),\n",
    "        'min': np.min(values),\n",
    "        'max': np.max(values),\n",
    "        'median': np.median(values)\n",
    "    }\n",
    "\n",
    "# Add FID to statistics\n",
    "statistics['fid'] = {\n",
    "    'mean': fid_score,\n",
    "    'std': 0,\n",
    "    'min': fid_score,\n",
    "    'max': fid_score,\n",
    "    'median': fid_score\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \"*20 + \"QUANTITATIVE RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<15} {'Mean':<12} {'Std':<12} {'Min':<12} {'Max':<12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "metric_display = {\n",
    "    'psnr': 'PSNR (dB)',\n",
    "    'ssim': 'SSIM',\n",
    "    'lpips': 'LPIPS',\n",
    "    'mae': 'MAE',\n",
    "    'hist_sim': 'Hist Corr',\n",
    "    'fid': 'FID'\n",
    "}\n",
    "\n",
    "for metric_name, display_name in metric_display.items():\n",
    "    stats = statistics[metric_name]\n",
    "    print(f\"{display_name:<15} {stats['mean']:<12.4f} {stats['std']:<12.4f} \"\n",
    "          f\"{stats['min']:<12.4f} {stats['max']:<12.4f}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(statistics).T\n",
    "df.columns = ['Mean', 'Std', 'Min', 'Max', 'Median']\n",
    "csv_path = f'{result_dir}/quantitative_results.csv'\n",
    "df.to_csv(csv_path)\n",
    "print(f\"\\n✓ Results saved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f566ede7",
   "metadata": {},
   "source": [
    "## 11. Visualize Metric Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230e8030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Distribution of Image Quality Metrics (IHC → HE Translation)', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "metrics_info = [\n",
    "    ('psnr', 'PSNR (dB)', 'Higher is better'),\n",
    "    ('ssim', 'SSIM', 'Higher is better'),\n",
    "    ('lpips', 'LPIPS', 'Lower is better'),\n",
    "    ('mae', 'MAE', 'Lower is better'),\n",
    "    ('hist_sim', 'Histogram Correlation', 'Higher is better')\n",
    "]\n",
    "\n",
    "for idx, (metric_name, label, interpretation) in enumerate(metrics_info):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    values = results[metric_name]\n",
    "    \n",
    "    # Histogram with KDE\n",
    "    ax.hist(values, bins=30, alpha=0.6, color='skyblue', edgecolor='black', density=True)\n",
    "    \n",
    "    # KDE line\n",
    "    kde = gaussian_kde(values)\n",
    "    x_range = np.linspace(min(values), max(values), 100)\n",
    "    ax.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')\n",
    "    \n",
    "    # Mean and median lines\n",
    "    mean_val = statistics[metric_name]['mean']\n",
    "    median_val = statistics[metric_name]['median']\n",
    "    ax.axvline(mean_val, color='green', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.4f}')\n",
    "    ax.axvline(median_val, color='orange', linestyle='--', linewidth=2, label=f'Median: {median_val:.4f}')\n",
    "    \n",
    "    ax.set_xlabel(label, fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Density', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{label}\\n({interpretation})', fontsize=12)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# FID display in last subplot\n",
    "axes[1, 2].axis('off')\n",
    "axes[1, 2].text(0.5, 0.5, f'FID Score\\n{fid_score:.2f}', \n",
    "                ha='center', va='center', fontsize=24, fontweight='bold',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = f'{result_dir}/visualizations/metric_distributions.png'\n",
    "plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Metric distributions saved to: {fig_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac50b5d7",
   "metadata": {},
   "source": [
    "## 12. Color Histogram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e3ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize color histograms\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "fig.suptitle('Color Histogram Comparison: Generated vs Real HE', fontsize=14, fontweight='bold')\n",
    "\n",
    "colors = ['red', 'green', 'blue']\n",
    "channel_names = ['Red', 'Green', 'Blue']\n",
    "\n",
    "for ch_idx in range(3):\n",
    "    ax = axes[ch_idx]\n",
    "    \n",
    "    # Calculate average histogram\n",
    "    real_hist = np.zeros(256)\n",
    "    gen_hist = np.zeros(256)\n",
    "    \n",
    "    for real_img, gen_img in zip(real_he_images, generated_images):\n",
    "        h1, _ = np.histogram(real_img[ch_idx].flatten(), bins=256, range=(0, 1))\n",
    "        h2, _ = np.histogram(gen_img[ch_idx].flatten(), bins=256, range=(0, 1))\n",
    "        real_hist += h1\n",
    "        gen_hist += h2\n",
    "    \n",
    "    real_hist = real_hist / real_hist.sum()\n",
    "    gen_hist = gen_hist / gen_hist.sum()\n",
    "    \n",
    "    x = np.linspace(0, 1, 256)\n",
    "    ax.plot(x, real_hist, color=colors[ch_idx], linewidth=2, label='Real HE', alpha=0.7)\n",
    "    ax.plot(x, gen_hist, color=colors[ch_idx], linewidth=2, label='Generated HE', \n",
    "            linestyle='--', alpha=0.7)\n",
    "    ax.set_title(f'{channel_names[ch_idx]} Channel', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Intensity', fontsize=10)\n",
    "    ax.set_ylabel('Frequency', fontsize=10)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "hist_path = f'{result_dir}/visualizations/color_histograms.png'\n",
    "plt.savefig(hist_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Color histogram saved to: {hist_path}\")\n",
    "print(f\"\\nMean Histogram Correlation: {statistics['hist_sim']['mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a30858",
   "metadata": {},
   "source": [
    "## 13. Qualitative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1027cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 8 random samples\n",
    "num_samples = 8\n",
    "fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n",
    "fig.suptitle('Qualitative Results: IHC → HE Translation', fontsize=18, fontweight='bold')\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Input IHC\n",
    "    axes[i, 0].imshow(input_ihc_images[i].permute(1, 2, 0).numpy())\n",
    "    axes[i, 0].set_title('Input (IHC)', fontsize=12, fontweight='bold')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Generated HE\n",
    "    axes[i, 1].imshow(generated_images[i].permute(1, 2, 0).numpy())\n",
    "    axes[i, 1].set_title('Generated (HE)', fontsize=12, fontweight='bold')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Real HE\n",
    "    axes[i, 2].imshow(real_he_images[i].permute(1, 2, 0).numpy())\n",
    "    axes[i, 2].set_title('Ground Truth (HE)', fontsize=12, fontweight='bold')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "qual_path = f'{result_dir}/visualizations/qualitative_results.png'\n",
    "plt.savefig(qual_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Qualitative results saved to: {qual_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a10983",
   "metadata": {},
   "source": [
    "## 14. Best and Worst Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f64c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best and worst based on SSIM\n",
    "ssim_values = np.array(results['ssim'])\n",
    "psnr_values = np.array(results['psnr'])\n",
    "\n",
    "best_indices = np.argsort(ssim_values)[-4:][::-1]\n",
    "worst_indices = np.argsort(ssim_values)[:4]\n",
    "\n",
    "# Best cases\n",
    "fig, axes = plt.subplots(4, 3, figsize=(12, 16))\n",
    "fig.suptitle('Best Translation Results (Highest SSIM)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, sample_idx in enumerate(best_indices):\n",
    "    axes[idx, 0].imshow(input_ihc_images[sample_idx].permute(1, 2, 0).numpy())\n",
    "    axes[idx, 0].set_title('Input IHC', fontsize=11)\n",
    "    axes[idx, 0].axis('off')\n",
    "    \n",
    "    axes[idx, 1].imshow(generated_images[sample_idx].permute(1, 2, 0).numpy())\n",
    "    axes[idx, 1].set_title('Generated HE', fontsize=11)\n",
    "    axes[idx, 1].axis('off')\n",
    "    \n",
    "    axes[idx, 2].imshow(real_he_images[sample_idx].permute(1, 2, 0).numpy())\n",
    "    axes[idx, 2].set_title('Ground Truth HE', fontsize=11)\n",
    "    axes[idx, 2].axis('off')\n",
    "    \n",
    "    metrics_text = f'SSIM: {ssim_values[sample_idx]:.4f}\\nPSNR: {psnr_values[sample_idx]:.2f} dB'\n",
    "    axes[idx, 0].text(-0.15, 0.5, metrics_text, transform=axes[idx, 0].transAxes,\n",
    "                     fontsize=10, fontweight='bold', verticalalignment='center',\n",
    "                     bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "best_path = f'{result_dir}/visualizations/best_cases.png'\n",
    "plt.savefig(best_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Worst cases\n",
    "fig, axes = plt.subplots(4, 3, figsize=(12, 16))\n",
    "fig.suptitle('Worst Translation Results (Lowest SSIM)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, sample_idx in enumerate(worst_indices):\n",
    "    axes[idx, 0].imshow(input_ihc_images[sample_idx].permute(1, 2, 0).numpy())\n",
    "    axes[idx, 0].set_title('Input IHC', fontsize=11)\n",
    "    axes[idx, 0].axis('off')\n",
    "    \n",
    "    axes[idx, 1].imshow(generated_images[sample_idx].permute(1, 2, 0).numpy())\n",
    "    axes[idx, 1].set_title('Generated HE', fontsize=11)\n",
    "    axes[idx, 1].axis('off')\n",
    "    \n",
    "    axes[idx, 2].imshow(real_he_images[sample_idx].permute(1, 2, 0).numpy())\n",
    "    axes[idx, 2].set_title('Ground Truth HE', fontsize=11)\n",
    "    axes[idx, 2].axis('off')\n",
    "    \n",
    "    metrics_text = f'SSIM: {ssim_values[sample_idx]:.4f}\\nPSNR: {psnr_values[sample_idx]:.2f} dB'\n",
    "    axes[idx, 0].text(-0.15, 0.5, metrics_text, transform=axes[idx, 0].transAxes,\n",
    "                     fontsize=10, fontweight='bold', verticalalignment='center',\n",
    "                     bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "worst_path = f'{result_dir}/visualizations/worst_cases.png'\n",
    "plt.savefig(worst_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Best cases saved to: {best_path}\")\n",
    "print(f\"✓ Worst cases saved to: {worst_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9bafbf",
   "metadata": {},
   "source": [
    "## 15. Generate Paper-Ready Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create paper-ready table\n",
    "paper_table = pd.DataFrame({\n",
    "    'Metric': ['PSNR (dB) ↑', 'SSIM ↑', 'LPIPS ↓', 'MAE ↓', 'Hist Corr ↑', 'FID ↓'],\n",
    "    'Mean ± Std': [\n",
    "        f\"{statistics['psnr']['mean']:.2f} ± {statistics['psnr']['std']:.2f}\",\n",
    "        f\"{statistics['ssim']['mean']:.4f} ± {statistics['ssim']['std']:.4f}\",\n",
    "        f\"{statistics['lpips']['mean']:.4f} ± {statistics['lpips']['std']:.4f}\",\n",
    "        f\"{statistics['mae']['mean']:.4f} ± {statistics['mae']['std']:.4f}\",\n",
    "        f\"{statistics['hist_sim']['mean']:.4f} ± {statistics['hist_sim']['std']:.4f}\",\n",
    "        f\"{fid_score:.2f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \"*20 + \"PAPER-READY RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(paper_table.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "print(\"Note: ↑ = higher is better, ↓ = lower is better\")\n",
    "\n",
    "# Save LaTeX table\n",
    "latex_table = paper_table.to_latex(index=False, escape=False)\n",
    "latex_path = f'{result_dir}/paper_table.tex'\n",
    "with open(latex_path, 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(f\"\\n✓ LaTeX table saved to: {latex_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf078e9",
   "metadata": {},
   "source": [
    "## 16. Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299d70a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "report = f\"\"\"\n",
    "{'='*80}\n",
    "                    CYCLEGAN VIRTUAL STAINING TEST REPORT\n",
    "                        IHC to HE Translation Evaluation\n",
    "{'='*80}\n",
    "\n",
    "TEST CONFIGURATION\n",
    "{'-'*80}\n",
    "Model Path:           {model_path}\n",
    "Test Samples:         {len(test_dataset)}\n",
    "Image Size:           {params['input_size']}x{params['input_size']}\n",
    "Device:               {device}\n",
    "Model Epoch:          {params['model_epoch']}\n",
    "\n",
    "QUANTITATIVE RESULTS (Standard Virtual Staining Metrics)\n",
    "{'-'*80}\n",
    "Metric                Mean ± Std              Range\n",
    "{'-'*80}\n",
    "PSNR (dB)            {statistics['psnr']['mean']:.2f} ± {statistics['psnr']['std']:.2f}           [{statistics['psnr']['min']:.2f}, {statistics['psnr']['max']:.2f}]\n",
    "SSIM                 {statistics['ssim']['mean']:.4f} ± {statistics['ssim']['std']:.4f}         [{statistics['ssim']['min']:.4f}, {statistics['ssim']['max']:.4f}]\n",
    "LPIPS                {statistics['lpips']['mean']:.4f} ± {statistics['lpips']['std']:.4f}         [{statistics['lpips']['min']:.4f}, {statistics['lpips']['max']:.4f}]\n",
    "MAE                  {statistics['mae']['mean']:.4f} ± {statistics['mae']['std']:.4f}         [{statistics['mae']['min']:.4f}, {statistics['mae']['max']:.4f}]\n",
    "Histogram Corr       {statistics['hist_sim']['mean']:.4f} ± {statistics['hist_sim']['std']:.4f}         [{statistics['hist_sim']['min']:.4f}, {statistics['hist_sim']['max']:.4f}]\n",
    "FID Score            {fid_score:.2f}\n",
    "{'-'*80}\n",
    "\n",
    "INTERPRETATION\n",
    "{'-'*80}\n",
    "PSNR: {'Excellent' if statistics['psnr']['mean'] > 25 else 'Good' if statistics['psnr']['mean'] > 20 else 'Poor'} (target: >25 dB)\n",
    "SSIM: {'Excellent' if statistics['ssim']['mean'] > 0.8 else 'Good' if statistics['ssim']['mean'] > 0.7 else 'Poor'} (target: >0.8)\n",
    "FID:  {'Excellent' if fid_score < 50 else 'Good' if fid_score < 100 else 'Moderate' if fid_score < 200 else 'Poor'} (target: <50)\n",
    "Color Match: {'Excellent' if statistics['hist_sim']['mean'] > 0.9 else 'Good' if statistics['hist_sim']['mean'] > 0.8 else 'Moderate'} (target: >0.9)\n",
    "\n",
    "OUTPUT FILES\n",
    "{'-'*80}\n",
    "✓ Quantitative results:     {result_dir}/quantitative_results.csv\n",
    "✓ LaTeX table:             {result_dir}/paper_table.tex\n",
    "✓ Metric distributions:    {result_dir}/visualizations/metric_distributions.png\n",
    "✓ Color histograms:        {result_dir}/visualizations/color_histograms.png\n",
    "✓ Qualitative results:     {result_dir}/visualizations/qualitative_results.png\n",
    "✓ Best cases:              {result_dir}/visualizations/best_cases.png\n",
    "✓ Worst cases:             {result_dir}/visualizations/worst_cases.png\n",
    "\n",
    "{'='*80}\n",
    "                    EVALUATION COMPLETED SUCCESSFULLY\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "report_path = f'{result_dir}/evaluation_report.txt'\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"✓ Full report saved to: {report_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urban",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
