{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792535ab",
   "metadata": {},
   "source": [
    "# CycleGAN Test: IHC to HE Virtual Staining Evaluation\n",
    "\n",
    "This notebook provides comprehensive evaluation of IHC → HE translation using trained CycleGAN model.\n",
    "\n",
    "## Evaluation Metrics (Appropriate for Unpaired Virtual Staining)\n",
    "\n",
    "**Note**: IHC and HE are from consecutive tissue sections, NOT pixel-aligned. \n",
    "Therefore, pixel-wise metrics (PSNR, SSIM, MAE) are inappropriate.\n",
    "\n",
    "### Distribution-based Metrics (Used):\n",
    "- **FID**: Fréchet Inception Distance - Primary metric for GAN evaluation\n",
    "- **Inception Score (IS)**: Measures quality and diversity of generated images\n",
    "- **Color Histogram Similarity**: Critical for staining quality assessment\n",
    "- **Texture Metrics**: GLCM-based Haralick features for tissue structure analysis\n",
    "\n",
    "### NOT Used (Require pixel alignment):\n",
    "- ~~PSNR, SSIM, MAE~~ - Inappropriate for unpaired images\n",
    "- ~~LPIPS~~ - Assumes spatial correspondence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419d345",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f471350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models import inception_v3\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "from scipy.stats import gaussian_kde, entropy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from skimage.color import rgb2gray\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95cd411",
   "metadata": {},
   "source": [
    "## 2. Test Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d480359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parameters\n",
    "params = {\n",
    "    'batch_size': 1,\n",
    "    'input_size': 512,\n",
    "    'test_sample_count': 100,\n",
    "    'model_epoch': 99,\n",
    "    'img_form': 'png'\n",
    "}\n",
    "\n",
    "# Paths\n",
    "data_dir = '../../data/IHC_HE_Pair_Data_GA_SS/patches'\n",
    "model_dir = '../../model/HE_IHC_translation/internal_ss/PD-L1'\n",
    "result_dir = '../../results/HE_IHC_translation/internal_ss/PD-L1/test_results'\n",
    "\n",
    "# Create directories\n",
    "create_dir(result_dir)\n",
    "create_dir(f'{result_dir}/visualizations')\n",
    "create_dir(f'{result_dir}/generated_images')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model epoch: {params['model_epoch']}\")\n",
    "print(f\"Test samples: {params['test_sample_count']}\")\n",
    "print(f\"Image size: {params['input_size']}x{params['input_size']}\")\n",
    "print(f\"Results directory: {result_dir}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d57b16",
   "metadata": {},
   "source": [
    "## 3. Define Generator Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d80b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(features, features, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(features, features, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, n_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "        model = [\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=7, padding=3),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        # Downsampling\n",
    "        in_features = 64\n",
    "        out_features = in_features * 2\n",
    "        for _ in range(4):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(4):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "\n",
    "        # Output layer\n",
    "        model += [nn.Conv2d(64, output_channels, kernel_size=7, padding=3), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "print(\"✓ Generator architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30289a69",
   "metadata": {},
   "source": [
    "## 4. Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3743a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, HE_image_list, IHC_image_list):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.HE_image_list = HE_image_list\n",
    "        self.IHC_image_list = IHC_image_list\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.HE_image_list[index], self.IHC_image_list[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.HE_image_list)\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=params['input_size']),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load test data\n",
    "print(\"Loading test data...\")\n",
    "test_data_HE = glob(f'{data_dir}/he_mpp1/*.{params[\"img_form\"]}')\n",
    "test_max_count = min(params['test_sample_count'], len(test_data_HE))\n",
    "test_data_HE = random.sample(test_data_HE, test_max_count)\n",
    "test_data_IHC = [f.replace('/he_mpp1/', '/pdl1_mpp1/') for f in test_data_HE]\n",
    "\n",
    "print(f\"Found {len(test_data_HE)} test image pairs\")\n",
    "\n",
    "# Preload images\n",
    "test_image_HE = torch.zeros((len(test_data_HE), 3, params['input_size'], params['input_size']))\n",
    "test_image_IHC = torch.zeros((len(test_data_IHC), 3, params['input_size'], params['input_size']))\n",
    "\n",
    "for i in tqdm(range(len(test_data_HE)), desc=\"Loading test images\"):\n",
    "    img = Image.open(test_data_HE[i]).convert('RGB')\n",
    "    target = Image.open(test_data_IHC[i]).convert('RGB')\n",
    "    img = transform(img) * 2. - 1\n",
    "    target = transform(target) * 2. - 1\n",
    "    test_image_HE[i] = img\n",
    "    test_image_IHC[i] = target\n",
    "\n",
    "test_dataset = DatasetFromFolder(test_image_HE, test_image_IHC)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=params['batch_size'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"✓ Test dataset loaded: {len(test_dataset)} image pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab036431",
   "metadata": {},
   "source": [
    "## 5. Load Trained Model (Generator F: IHC → HE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3084e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and load model\n",
    "F = Generator(3, 3).to(device)\n",
    "model_path = f'{model_dir}/F_{params[\"model_epoch\"]}.pth'\n",
    "F.load_state_dict(torch.load(model_path, map_location=device))\n",
    "F.eval()\n",
    "\n",
    "print(f\"✓ Loaded model: {model_path}\")\n",
    "print(f\"✓ Generator F (IHC → HE) ready for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17e71be",
   "metadata": {},
   "source": [
    "## 6. Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0977e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(img):\n",
    "    \"\"\"Denormalize from [-1, 1] to [0, 1]\"\"\"\n",
    "    return img * 0.5 + 0.5\n",
    "\n",
    "def calculate_histogram_similarity(img1, img2, bins=256):\n",
    "    \"\"\"Calculate histogram correlation for RGB channels\"\"\"\n",
    "    correlations = []\n",
    "    for channel in range(3):\n",
    "        hist1, _ = np.histogram(img1[channel].flatten(), bins=bins, range=(0, 1))\n",
    "        hist2, _ = np.histogram(img2[channel].flatten(), bins=bins, range=(0, 1))\n",
    "        hist1 = hist1 / hist1.sum()\n",
    "        hist2 = hist2 / hist2.sum()\n",
    "        correlation = np.corrcoef(hist1, hist2)[0, 1]\n",
    "        correlations.append(correlation)\n",
    "    return np.mean(correlations)\n",
    "\n",
    "def calculate_haralick_features(img):\n",
    "    \"\"\"Calculate GLCM-based Haralick texture features\"\"\"\n",
    "    # Convert to grayscale and scale to [0, 255]\n",
    "    if img.shape[0] == 3:  # CHW format\n",
    "        gray = rgb2gray(img.transpose(1, 2, 0))\n",
    "    else:\n",
    "        gray = img\n",
    "    \n",
    "    gray_uint8 = (gray * 255).astype(np.uint8)\n",
    "    \n",
    "    # Calculate GLCM for 4 directions\n",
    "    distances = [1]\n",
    "    angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    glcm = graycomatrix(gray_uint8, distances=distances, angles=angles, \n",
    "                        levels=256, symmetric=True, normed=True)\n",
    "    \n",
    "    # Calculate Haralick features\n",
    "    features = {\n",
    "        'contrast': graycoprops(glcm, 'contrast').mean(),\n",
    "        'dissimilarity': graycoprops(glcm, 'dissimilarity').mean(),\n",
    "        'homogeneity': graycoprops(glcm, 'homogeneity').mean(),\n",
    "        'energy': graycoprops(glcm, 'energy').mean(),\n",
    "        'correlation': graycoprops(glcm, 'correlation').mean(),\n",
    "        'ASM': graycoprops(glcm, 'ASM').mean()\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def calculate_texture_similarity(img1, img2):\n",
    "    \"\"\"Calculate texture similarity based on Haralick features\"\"\"\n",
    "    features1 = calculate_haralick_features(img1)\n",
    "    features2 = calculate_haralick_features(img2)\n",
    "    \n",
    "    # Calculate normalized L2 distance\n",
    "    diff_sum = 0\n",
    "    norm_sum = 0\n",
    "    for key in features1.keys():\n",
    "        diff = (features1[key] - features2[key]) ** 2\n",
    "        norm = (features1[key] + features2[key]) ** 2 + 1e-10\n",
    "        diff_sum += diff\n",
    "        norm_sum += norm\n",
    "    \n",
    "    # Convert to similarity score (0 to 1, higher is better)\n",
    "    similarity = 1 - np.sqrt(diff_sum / norm_sum)\n",
    "    return similarity\n",
    "\n",
    "print(\"✓ Evaluation metrics initialized\")\n",
    "print(\"  - Color Histogram Similarity (distribution-based)\")\n",
    "print(\"  - Texture Similarity (GLCM Haralick features)\")\n",
    "print(\"  - FID (will be calculated separately)\")\n",
    "print(\"  - Inception Score (will be calculated separately)\")\n",
    "print(\"\\nNote: Pixel-wise metrics (PSNR, SSIM, MAE, LPIPS) are NOT used\")\n",
    "print(\"      because IHC and HE images are from consecutive sections\")\n",
    "print(\"      and are NOT pixel-aligned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25b430",
   "metadata": {},
   "source": [
    "## 7. Run Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f04e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for metrics\n",
    "results = {\n",
    "    'hist_sim': [],\n",
    "    'texture_sim': []\n",
    "}\n",
    "\n",
    "# Storage for ALL generated images (needed for FID and visualization)\n",
    "generated_images = []\n",
    "real_he_images = []\n",
    "input_ihc_images = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING EVALUATION ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (real_he, real_ihc) in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n",
    "        real_he = real_he.to(device)\n",
    "        real_ihc = real_ihc.to(device)\n",
    "        \n",
    "        # Generate fake HE from IHC\n",
    "        fake_he = F(real_ihc)\n",
    "        \n",
    "        # Denormalize\n",
    "        real_he_denorm = denormalize(real_he[0])\n",
    "        fake_he_denorm = denormalize(fake_he[0])\n",
    "        \n",
    "        # Calculate color histogram similarity (distribution-based)\n",
    "        results['hist_sim'].append(calculate_histogram_similarity(fake_he_denorm.cpu(), real_he_denorm.cpu()))\n",
    "        \n",
    "        # Calculate texture similarity (GLCM Haralick features)\n",
    "        results['texture_sim'].append(calculate_texture_similarity(\n",
    "            fake_he_denorm.cpu().numpy(), \n",
    "            real_he_denorm.cpu().numpy()\n",
    "        ))\n",
    "        \n",
    "        # Store all images\n",
    "        generated_images.append(fake_he_denorm.cpu())\n",
    "        real_he_images.append(real_he_denorm.cpu())\n",
    "        input_ihc_images.append(denormalize(real_ihc[0]).cpu())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Evaluation completed!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a529a3f",
   "metadata": {},
   "source": [
    "## 8. Calculate FID Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6629a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import sqrtm\n",
    "\n",
    "# Load Inception V3\n",
    "print(\"Loading Inception V3 model for FID calculation...\")\n",
    "inception_model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "inception_model.fc = nn.Identity()\n",
    "inception_model.eval()\n",
    "\n",
    "def get_inception_features(images):\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for img in tqdm(images, desc=\"Extracting features\"):\n",
    "            # Resize to 299x299 for Inception\n",
    "            img_resized = torch.nn.functional.interpolate(\n",
    "                img.unsqueeze(0), size=(299, 299), mode='bilinear', align_corners=False\n",
    "            )\n",
    "            # Normalize to [-1, 1]\n",
    "            img_normalized = img_resized * 2 - 1\n",
    "            feat = inception_model(img_normalized.to(device))\n",
    "            features.append(feat.cpu().numpy())\n",
    "    return np.concatenate(features, axis=0)\n",
    "\n",
    "def calculate_fid(features1, features2):\n",
    "    mu1, sigma1 = features1.mean(axis=0), np.cov(features1, rowvar=False)\n",
    "    mu2, sigma2 = features2.mean(axis=0), np.cov(features2, rowvar=False)\n",
    "    \n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid\n",
    "\n",
    "# Extract features\n",
    "print(\"Extracting features from real HE images...\")\n",
    "real_features = get_inception_features(real_he_images)\n",
    "\n",
    "print(\"Extracting features from generated HE images...\")\n",
    "fake_features = get_inception_features(generated_images)\n",
    "\n",
    "# Calculate FID scores\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FID Score Calculation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Randomly shuffle both real and fake features for fair comparison\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# 1. Real vs Real baseline (random split)\n",
    "real_indices = np.random.permutation(len(real_features))\n",
    "mid_point = len(real_features) // 2\n",
    "real_features_1 = real_features[real_indices[:mid_point]]\n",
    "real_features_2 = real_features[real_indices[mid_point:]]\n",
    "\n",
    "fid_real_vs_real = calculate_fid(real_features_1, real_features_2)\n",
    "print(f\"FID (Real vs Real - Baseline): {fid_real_vs_real:.2f}\")\n",
    "print(f\"  (Real features randomly shuffled)\")\n",
    "\n",
    "# 2. Fake vs Real (both randomly shuffled)\n",
    "fake_indices = np.random.permutation(len(fake_features))\n",
    "real_indices_full = np.random.permutation(len(real_features))\n",
    "\n",
    "fake_features_shuffled = fake_features[fake_indices]\n",
    "real_features_shuffled = real_features[real_indices_full]\n",
    "\n",
    "fid_fake_vs_real = calculate_fid(fake_features_shuffled, real_features_shuffled)\n",
    "print(f\"\\nFID (Fake vs Real): {fid_fake_vs_real:.2f}\")\n",
    "print(f\"  (Both fake and real features randomly shuffled)\")\n",
    "\n",
    "# Calculate relative FID\n",
    "fid_difference = fid_fake_vs_real - fid_real_vs_real\n",
    "print(f\"\\nFID Difference (Fake-Real): {fid_difference:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Interpretation:\")\n",
    "print(f\"  - Baseline FID (Real vs Real): {fid_real_vs_real:.2f}\")\n",
    "print(f\"    Expected to be very low (<10), represents noise floor\")\n",
    "print(f\"  - Fake vs Real FID: {fid_fake_vs_real:.2f}\")\n",
    "if fid_fake_vs_real < 50:\n",
    "    print(\"    → Excellent quality\")\n",
    "elif fid_fake_vs_real < 100:\n",
    "    print(\"    → Good quality\")\n",
    "elif fid_fake_vs_real < 200:\n",
    "    print(\"    → Moderate quality\")\n",
    "else:\n",
    "    print(\"    → Poor quality\")\n",
    "print(f\"  - Relative FID increase: {fid_difference:.2f}\")\n",
    "print(\"    Lower is better (closer to baseline)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cacefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Inception Score Calculation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load Inception V3 for classification (not feature extraction)\n",
    "inception_classifier = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "inception_classifier.eval()\n",
    "\n",
    "def calculate_inception_score(images, batch_size=32, splits=10):\n",
    "    \"\"\"\n",
    "    Calculate Inception Score for generated images\n",
    "    IS = exp(E[KL(p(y|x) || p(y))])\n",
    "    Higher is better (measures quality and diversity)\n",
    "    \"\"\"\n",
    "    n_images = len(images)\n",
    "    \n",
    "    # Get predictions\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, n_images, batch_size), desc=\"Computing IS\"):\n",
    "            batch = images[i:i+batch_size]\n",
    "            batch_tensor = torch.stack([img for img in batch])\n",
    "            \n",
    "            # Resize to 299x299 for Inception\n",
    "            batch_resized = torch.nn.functional.interpolate(\n",
    "                batch_tensor, size=(299, 299), mode='bilinear', align_corners=False\n",
    "            )\n",
    "            # Normalize to [-1, 1]\n",
    "            batch_normalized = batch_resized * 2 - 1\n",
    "            \n",
    "            # Get class probabilities\n",
    "            pred = torch.nn.functional.softmax(inception_classifier(batch_normalized.to(device)), dim=1)\n",
    "            preds.append(pred.cpu().numpy())\n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    \n",
    "    # Calculate IS\n",
    "    split_scores = []\n",
    "    for k in range(splits):\n",
    "        part = preds[k * (n_images // splits): (k + 1) * (n_images // splits), :]\n",
    "        py = np.mean(part, axis=0)\n",
    "        scores = []\n",
    "        for i in range(part.shape[0]):\n",
    "            pyx = part[i, :]\n",
    "            scores.append(entropy(pyx, py))\n",
    "        split_scores.append(np.exp(np.mean(scores)))\n",
    "    \n",
    "    return np.mean(split_scores), np.std(split_scores)\n",
    "\n",
    "# Calculate IS for generated images\n",
    "is_mean, is_std = calculate_inception_score(generated_images)\n",
    "\n",
    "print(f\"\\nInception Score: {is_mean:.2f} ± {is_std:.2f}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "if is_mean > 8:\n",
    "    print(\"  → Excellent quality and diversity\")\n",
    "elif is_mean > 5:\n",
    "    print(\"  → Good quality and diversity\")\n",
    "elif is_mean > 3:\n",
    "    print(\"  → Moderate quality\")\n",
    "else:\n",
    "    print(\"  → Poor quality or low diversity\")\n",
    "print(\"\\nNote: Higher IS indicates better image quality and diversity\")\n",
    "print(\"      Real images typically have IS > 10\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5162b62d",
   "metadata": {},
   "source": [
    "## 10. Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d421f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "statistics = {}\n",
    "for metric_name, values in results.items():\n",
    "    statistics[metric_name] = {\n",
    "        'mean': np.mean(values),\n",
    "        'std': np.std(values),\n",
    "        'min': np.min(values),\n",
    "        'max': np.max(values),\n",
    "        'median': np.median(values)\n",
    "    }\n",
    "\n",
    "# Add FID scores to statistics\n",
    "statistics['fid_baseline'] = {\n",
    "    'mean': fid_real_vs_real,\n",
    "    'std': 0,\n",
    "    'min': fid_real_vs_real,\n",
    "    'max': fid_real_vs_real,\n",
    "    'median': fid_real_vs_real\n",
    "}\n",
    "\n",
    "statistics['fid_fake_real'] = {\n",
    "    'mean': fid_fake_vs_real,\n",
    "    'std': 0,\n",
    "    'min': fid_fake_vs_real,\n",
    "    'max': fid_fake_vs_real,\n",
    "    'median': fid_fake_vs_real\n",
    "}\n",
    "\n",
    "statistics['fid_difference'] = {\n",
    "    'mean': fid_difference,\n",
    "    'std': 0,\n",
    "    'min': fid_difference,\n",
    "    'max': fid_difference,\n",
    "    'median': fid_difference\n",
    "}\n",
    "\n",
    "# Add Inception Score to statistics\n",
    "statistics['inception_score'] = {\n",
    "    'mean': is_mean,\n",
    "    'std': is_std,\n",
    "    'min': is_mean,\n",
    "    'max': is_mean,\n",
    "    'median': is_mean\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \"*25 + \"QUANTITATIVE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Metric':<20} {'Mean':<12} {'Std':<12} {'Min':<12} {'Max':<12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "metric_display = {\n",
    "    'hist_sim': 'Hist Corr',\n",
    "    'texture_sim': 'Texture Sim',\n",
    "    'inception_score': 'Inception Score',\n",
    "    'fid_baseline': 'FID (Real-Real)',\n",
    "    'fid_fake_real': 'FID (Fake-Real)',\n",
    "    'fid_difference': 'FID Difference'\n",
    "}\n",
    "\n",
    "for metric_name, display_name in metric_display.items():\n",
    "    stats = statistics[metric_name]\n",
    "    print(f\"{display_name:<20} {stats['mean']:<12.4f} {stats['std']:<12.4f} \"\n",
    "          f\"{stats['min']:<12.4f} {stats['max']:<12.4f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNote: Pixel-wise metrics (PSNR, SSIM, MAE, LPIPS) are NOT reported\")\n",
    "print(\"      because IHC and HE are from consecutive sections (unpaired).\")\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(statistics).T\n",
    "df.columns = ['Mean', 'Std', 'Min', 'Max', 'Median']\n",
    "csv_path = f'{result_dir}/quantitative_results.csv'\n",
    "df.to_csv(csv_path)\n",
    "print(f\"\\n✓ Results saved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f566ede7",
   "metadata": {},
   "source": [
    "## 11. Visualize Metric Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230e8030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create subplots for distribution metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Distribution-Based Metrics (IHC → HE Translation)', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "# Histogram Correlation distribution\n",
    "ax = axes[0]\n",
    "values = results['hist_sim']\n",
    "ax.hist(values, bins=30, alpha=0.6, color='skyblue', edgecolor='black', density=True)\n",
    "kde = gaussian_kde(values)\n",
    "x_range = np.linspace(min(values), max(values), 100)\n",
    "ax.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')\n",
    "mean_val = statistics['hist_sim']['mean']\n",
    "median_val = statistics['hist_sim']['median']\n",
    "ax.axvline(mean_val, color='green', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.4f}')\n",
    "ax.axvline(median_val, color='orange', linestyle='--', linewidth=2, label=f'Median: {median_val:.4f}')\n",
    "ax.set_xlabel('Histogram Correlation', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Density', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Color Histogram Correlation\\n(Higher is better)', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Texture Similarity distribution\n",
    "ax = axes[1]\n",
    "values = results['texture_sim']\n",
    "ax.hist(values, bins=30, alpha=0.6, color='lightcoral', edgecolor='black', density=True)\n",
    "kde = gaussian_kde(values)\n",
    "x_range = np.linspace(min(values), max(values), 100)\n",
    "ax.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')\n",
    "mean_val = statistics['texture_sim']['mean']\n",
    "median_val = statistics['texture_sim']['median']\n",
    "ax.axvline(mean_val, color='green', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.4f}')\n",
    "ax.axvline(median_val, color='orange', linestyle='--', linewidth=2, label=f'Median: {median_val:.4f}')\n",
    "ax.set_xlabel('Texture Similarity (GLCM)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Density', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Texture Similarity (Haralick Features)\\n(Higher is better)', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = f'{result_dir}/visualizations/metric_distributions.png'\n",
    "plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Create separate figure for FID and IS summary\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.axis('off')\n",
    "\n",
    "summary_text = 'GAN Evaluation Metrics Summary\\n\\n'\n",
    "summary_text += f'FID Scores:\\n'\n",
    "summary_text += f'  Real vs Real (Baseline): {fid_real_vs_real:.2f}\\n'\n",
    "summary_text += f'  Fake vs Real: {fid_fake_vs_real:.2f}\\n'\n",
    "summary_text += f'  Difference: {fid_difference:.2f}\\n\\n'\n",
    "if fid_fake_vs_real < 50:\n",
    "    summary_text += '  FID Quality: Excellent\\n\\n'\n",
    "elif fid_fake_vs_real < 100:\n",
    "    summary_text += '  FID Quality: Good\\n\\n'\n",
    "elif fid_fake_vs_real < 200:\n",
    "    summary_text += '  FID Quality: Moderate\\n\\n'\n",
    "else:\n",
    "    summary_text += '  FID Quality: Poor\\n\\n'\n",
    "\n",
    "summary_text += f'Inception Score:\\n'\n",
    "summary_text += f'  IS: {is_mean:.2f} ± {is_std:.2f}\\n'\n",
    "if is_mean > 8:\n",
    "    summary_text += '  IS Quality: Excellent\\n'\n",
    "elif is_mean > 5:\n",
    "    summary_text += '  IS Quality: Good\\n'\n",
    "elif is_mean > 3:\n",
    "    summary_text += '  IS Quality: Moderate\\n'\n",
    "else:\n",
    "    summary_text += '  IS Quality: Poor\\n'\n",
    "\n",
    "ax.text(0.5, 0.5, summary_text, transform=ax.transAxes,\n",
    "        ha='center', va='center', fontsize=14, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n",
    "        family='monospace')\n",
    "\n",
    "summary_path = f'{result_dir}/visualizations/gan_metrics_summary.png'\n",
    "plt.savefig(summary_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Metric distributions saved to: {fig_path}\")\n",
    "print(f\"✓ GAN metrics summary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac50b5d7",
   "metadata": {},
   "source": [
    "## 12. Color Histogram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e3ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize color histograms\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "fig.suptitle('Color Histogram Comparison: Generated vs Real HE', fontsize=14, fontweight='bold')\n",
    "\n",
    "colors = ['red', 'green', 'blue']\n",
    "channel_names = ['Red', 'Green', 'Blue']\n",
    "\n",
    "for ch_idx in range(3):\n",
    "    ax = axes[ch_idx]\n",
    "    \n",
    "    # Calculate average histogram\n",
    "    real_hist = np.zeros(256)\n",
    "    gen_hist = np.zeros(256)\n",
    "    \n",
    "    for real_img, gen_img in zip(real_he_images, generated_images):\n",
    "        h1, _ = np.histogram(real_img[ch_idx].flatten(), bins=256, range=(0, 1))\n",
    "        h2, _ = np.histogram(gen_img[ch_idx].flatten(), bins=256, range=(0, 1))\n",
    "        real_hist += h1\n",
    "        gen_hist += h2\n",
    "    \n",
    "    real_hist = real_hist / real_hist.sum()\n",
    "    gen_hist = gen_hist / gen_hist.sum()\n",
    "    \n",
    "    x = np.linspace(0, 1, 256)\n",
    "    ax.plot(x, real_hist, color=colors[ch_idx], linewidth=2, label='Real HE', alpha=0.7)\n",
    "    ax.plot(x, gen_hist, color=colors[ch_idx], linewidth=2, label='Generated HE', \n",
    "            linestyle='--', alpha=0.7)\n",
    "    ax.set_title(f'{channel_names[ch_idx]} Channel', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Intensity', fontsize=10)\n",
    "    ax.set_ylabel('Frequency', fontsize=10)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "hist_path = f'{result_dir}/visualizations/color_histograms.png'\n",
    "plt.savefig(hist_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Color histogram saved to: {hist_path}\")\n",
    "print(f\"\\nMean Histogram Correlation: {statistics['hist_sim']['mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a30858",
   "metadata": {},
   "source": [
    "## 13. Qualitative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1027cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 8 random samples\n",
    "num_samples = 8\n",
    "fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n",
    "fig.suptitle('Qualitative Results: IHC → HE Translation', fontsize=18, fontweight='bold')\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Input IHC\n",
    "    axes[i, 0].imshow(input_ihc_images[i].permute(1, 2, 0).numpy())\n",
    "    axes[i, 0].set_title('Input (IHC)', fontsize=12, fontweight='bold')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Generated HE\n",
    "    axes[i, 1].imshow(generated_images[i].permute(1, 2, 0).numpy())\n",
    "    axes[i, 1].set_title('Generated (HE)', fontsize=12, fontweight='bold')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Real HE\n",
    "    axes[i, 2].imshow(real_he_images[i].permute(1, 2, 0).numpy())\n",
    "    axes[i, 2].set_title('Ground Truth (HE)', fontsize=12, fontweight='bold')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "qual_path = f'{result_dir}/visualizations/qualitative_results.png'\n",
    "plt.savefig(qual_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Qualitative results saved to: {qual_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a10983",
   "metadata": {},
   "source": [
    "## 14. Best and Worst Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f64c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best and worst based on histogram correlation\n",
    "hist_values = np.array(results['hist_sim'])\n",
    "\n",
    "best_indices = np.argsort(hist_values)[-4:][::-1]\n",
    "worst_indices = np.argsort(hist_values)[:4]\n",
    "\n",
    "# Best cases\n",
    "fig, axes = plt.subplots(4, 3, figsize=(12, 16))\n",
    "fig.suptitle('Best Translation Results (Highest Histogram Correlation)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, sample_idx in enumerate(best_indices):\n",
    "    axes[idx, 0].imshow(input_ihc_images[sample_idx].permute(1, 2, 0).numpy())\n",
    "    axes[idx, 0].set_title('Input IHC', fontsize=11)\n",
    "    axes[idx, 0].axis('off')\n",
    "    \n",
    "    axes[idx, 1].imshow(generated_images[sample_idx].permute(1, 2, 0).numpy())\n",
    "    axes[idx, 1].set_title('Generated HE', fontsize=11)\n",
    "    axes[idx, 1].axis('off')\n",
    "    \n",
    "    axes[idx, 2].imshow(real_he_images[sample_idx].permute(1, 2, 0).numpy())\n",
    "    axes[idx, 2].set_title('Ground Truth HE', fontsize=11)\n",
    "    axes[idx, 2].axis('off')\n",
    "    \n",
    "    metrics_text = f'Hist Corr: {hist_values[sample_idx]:.4f}'\n",
    "    axes[idx, 0].text(-0.15, 0.5, metrics_text, transform=axes[idx, 0].transAxes,\n",
    "                     fontsize=10, fontweight='bold', verticalalignment='center',\n",
    "                     bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "best_path = f'{result_dir}/visualizations/best_cases.png'\n",
    "plt.savefig(best_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Worst cases\n",
    "fig, axes = plt.subplots(4, 3, figsize=(12, 16))\n",
    "fig.suptitle('Worst Translation Results (Lowest Histogram Correlation)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, sample_idx in enumerate(worst_indices):\n",
    "    axes[idx, 0].imshow(input_ihc_images[sample_idx].permute(1, 2, 0).numpy())\n",
    "    axes[idx, 0].set_title('Input IHC', fontsize=11)\n",
    "    axes[idx, 0].axis('off')\n",
    "    \n",
    "    axes[idx, 1].imshow(generated_images[sample_idx].permute(1, 2, 0).numpy())\n",
    "    axes[idx, 1].set_title('Generated HE', fontsize=11)\n",
    "    axes[idx, 1].axis('off')\n",
    "    \n",
    "    axes[idx, 2].imshow(real_he_images[sample_idx].permute(1, 2, 0).numpy())\n",
    "    axes[idx, 2].set_title('Ground Truth HE', fontsize=11)\n",
    "    axes[idx, 2].axis('off')\n",
    "    \n",
    "    metrics_text = f'Hist Corr: {hist_values[sample_idx]:.4f}'\n",
    "    axes[idx, 0].text(-0.15, 0.5, metrics_text, transform=axes[idx, 0].transAxes,\n",
    "                     fontsize=10, fontweight='bold', verticalalignment='center',\n",
    "                     bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "worst_path = f'{result_dir}/visualizations/worst_cases.png'\n",
    "plt.savefig(worst_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Best cases saved to: {best_path}\")\n",
    "print(f\"✓ Worst cases saved to: {worst_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9bafbf",
   "metadata": {},
   "source": [
    "## 15. Generate Paper-Ready Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create paper-ready table\n",
    "paper_table = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Hist Corr ↑', \n",
    "        'Texture Sim ↑', \n",
    "        'Inception Score ↑',\n",
    "        'FID (Fake-Real) ↓', \n",
    "        'FID (Baseline) ↓', \n",
    "        'FID Difference ↓'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{statistics['hist_sim']['mean']:.4f} ± {statistics['hist_sim']['std']:.4f}\",\n",
    "        f\"{statistics['texture_sim']['mean']:.4f} ± {statistics['texture_sim']['std']:.4f}\",\n",
    "        f\"{is_mean:.2f} ± {is_std:.2f}\",\n",
    "        f\"{fid_fake_vs_real:.2f}\",\n",
    "        f\"{fid_real_vs_real:.2f}\",\n",
    "        f\"{fid_difference:.2f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \"*20 + \"PAPER-READY RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(paper_table.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "print(\"Note: ↑ = higher is better, ↓ = lower is better\")\n",
    "print(\"\\nMetrics are distribution-based (appropriate for unpaired images)\")\n",
    "print(\"  - Color Histogram Correlation: Stain color distribution match\")\n",
    "print(\"  - Texture Similarity: GLCM Haralick features for tissue structure\")\n",
    "print(\"  - Inception Score: GAN generation quality and diversity\")\n",
    "print(\"  - FID: Fréchet distance in Inception feature space\")\n",
    "print(\"\\nPixel-wise metrics (PSNR, SSIM, MAE, LPIPS) are NOT applicable\")\n",
    "print(\"because IHC and HE are from consecutive tissue sections.\")\n",
    "\n",
    "# Save LaTeX table\n",
    "latex_table = paper_table.to_latex(index=False, escape=False)\n",
    "latex_path = f'{result_dir}/paper_table.tex'\n",
    "with open(latex_path, 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(f\"\\n✓ LaTeX table saved to: {latex_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf078e9",
   "metadata": {},
   "source": [
    "## 16. Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299d70a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "report = f\"\"\"\n",
    "{'='*80}\n",
    "                    CYCLEGAN VIRTUAL STAINING TEST REPORT\n",
    "                        IHC to HE Translation Evaluation\n",
    "{'='*80}\n",
    "\n",
    "TEST CONFIGURATION\n",
    "{'-'*80}\n",
    "Model Path:           {model_path}\n",
    "Test Samples:         {len(test_dataset)}\n",
    "Image Size:           {params['input_size']}x{params['input_size']}\n",
    "Device:               {device}\n",
    "Model Epoch:          {params['model_epoch']}\n",
    "\n",
    "IMPORTANT NOTE\n",
    "{'-'*80}\n",
    "IHC and HE images are from CONSECUTIVE TISSUE SECTIONS, not pixel-aligned.\n",
    "Therefore, only DISTRIBUTION-BASED metrics are reported:\n",
    "  ✓ FID (Fréchet Inception Distance) - Primary GAN metric\n",
    "  ✓ Inception Score (IS) - Quality and diversity measure\n",
    "  ✓ Color Histogram Correlation - Stain color distribution\n",
    "  ✓ Texture Similarity - GLCM Haralick features for tissue structure\n",
    "\n",
    "Pixel-wise metrics (PSNR, SSIM, MAE, LPIPS) are NOT applicable because\n",
    "they require spatial correspondence between images.\n",
    "\n",
    "QUANTITATIVE RESULTS\n",
    "{'-'*80}\n",
    "Metric                Value\n",
    "{'-'*80}\n",
    "Histogram Corr       {statistics['hist_sim']['mean']:.4f} ± {statistics['hist_sim']['std']:.4f}\n",
    "Texture Similarity   {statistics['texture_sim']['mean']:.4f} ± {statistics['texture_sim']['std']:.4f}\n",
    "Inception Score      {is_mean:.2f} ± {is_std:.2f}\n",
    "FID (Baseline)       {fid_real_vs_real:.2f}\n",
    "FID (Fake-Real)      {fid_fake_vs_real:.2f}\n",
    "FID Difference       {fid_difference:.2f}\n",
    "{'-'*80}\n",
    "\n",
    "INTERPRETATION\n",
    "{'-'*80}\n",
    "FID:  {'Excellent' if fid_fake_vs_real < 50 else 'Good' if fid_fake_vs_real < 100 else 'Moderate' if fid_fake_vs_real < 200 else 'Poor'} (target: <50)\n",
    "FID Baseline: {fid_real_vs_real:.2f} (noise floor)\n",
    "Inception Score: {'Excellent' if is_mean > 8 else 'Good' if is_mean > 5 else 'Moderate' if is_mean > 3 else 'Poor'} (real images: >10)\n",
    "Color Match: {'Excellent' if statistics['hist_sim']['mean'] > 0.9 else 'Good' if statistics['hist_sim']['mean'] > 0.8 else 'Moderate'} (target: >0.9)\n",
    "Texture Match: {'Excellent' if statistics['texture_sim']['mean'] > 0.9 else 'Good' if statistics['texture_sim']['mean'] > 0.8 else 'Moderate'} (target: >0.8)\n",
    "\n",
    "OUTPUT FILES\n",
    "{'-'*80}\n",
    "✓ Quantitative results:     {result_dir}/quantitative_results.csv\n",
    "✓ LaTeX table:             {result_dir}/paper_table.tex\n",
    "✓ Metric distributions:    {result_dir}/visualizations/metric_distributions.png\n",
    "✓ GAN metrics summary:     {result_dir}/visualizations/gan_metrics_summary.png\n",
    "✓ Color histograms:        {result_dir}/visualizations/color_histograms.png\n",
    "✓ Qualitative results:     {result_dir}/visualizations/qualitative_results.png\n",
    "✓ Best cases:              {result_dir}/visualizations/best_cases.png\n",
    "✓ Worst cases:             {result_dir}/visualizations/worst_cases.png\n",
    "\n",
    "{'='*80}\n",
    "                    EVALUATION COMPLETED SUCCESSFULLY\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "report_path = f'{result_dir}/evaluation_report.txt'\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"✓ Full report saved to: {report_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urban",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
